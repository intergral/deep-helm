# Default values for deep.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.
global:
  image:
    # define the default image config for all containers
    registry: docker.io
    repository: intergral/deep
    tag: null
    pullSecrets: []
    pullPolicy: IfNotPresent
  # -- Overrides the priorityClassName for all pods
  priorityClassName: null
  # -- configures cluster domain ("cluster.local" by default)
  clusterDomain: 'cluster.local'
  # -- configures DNS service name
  dnsService: 'kube-dns'
  # -- configures DNS service namespace
  dnsNamespace: 'kube-system'
  ## Node selector for all pods
  nodeSelector: {}

## Lets you override the name used for this chart, will default to 'deep'.
nameOverride: ""
## Lets you override the full name used by this chart.
fullnameOverride: ""
## Lets you set the namespace to use when deploying, or upgrading deep.
namespace: "deep"

deep:
  # -- Global labels for all deep pods
  podLabels: {}
  # -- Common annotations for all pods
  podAnnotations: {}
  # -- podSecurityContext holds pod-level security attributes and common container settings
  podSecurityContext:
    fsGroup: 1000
  readinessProbe:
    httpGet:
      path: /ready
      port: http-metrics
    initialDelaySeconds: 30
    timeoutSeconds: 1
  # -- SecurityContext holds container-level security attributes and common container settings
  securityContext:
    runAsNonRoot: true
    runAsUser: 1000
    runAsGroup: 1000
    allowPrivilegeEscalation: false
    capabilities:
      drop:
        - ALL
    readOnlyRootFilesystem: true
  memberlist:
    # -- Adds the appProtocol field to the memberlist service. This allows memberlist to work with
    # istio protocol selection. Set the optional service protocol. Ex: "tcp", "http" or "https".
    appProtocol: null

compactor:
  # -- Number of replicas for the compactor
  replicas: 1
  # -- hostAliases to add
  hostAliases: []
  #  - ip: 1.2.3.4
  #    hostnames:
  #      - domain.tld
  image:
    # -- The Docker registry for the compactor image. Overrides `deep.image.registry`
    registry: null
    # -- Optional list of imagePullSecrets. Overrides `deep.image.pullSecrets`
    pullSecrets: []
    # -- Docker image repository for the compactor image. Overrides `deep.image.repository`
    repository: null
    # -- Docker image tag for the compactor image. Overrides `deep.image.tag`
    tag: null
  # -- The name of the PriorityClass for compactor pods
  priorityClassName: null
  # -- Labels for compactor pods
  podLabels: {}
  # -- Annotations for compactor deployment
  annotations: {}
  # -- Annotations for compactor pods
  podAnnotations: {}
  # -- Additional CLI args for the compactor
  extraArgs: []
  # -- Environment variables to add to the compactor pods
  extraEnv: []
  # -- Environment variables from secrets or configmaps to add to the compactor pods
  extraEnvFrom: []
  # -- Resource requests and limits for the compactor
  resources: {}
  # -- Grace period to allow the compactor to shutdown before it is killed
  terminationGracePeriodSeconds: 30
  # -- Node selector for compactor pods
  nodeSelector: {}
  # -- Tolerations for compactor pods
  tolerations: []
  # -- Extra volumes for compactor pods
  extraVolumeMounts: []
  # -- Extra volumes for compactor deployment
  extraVolumes: []
  service:
    # -- Annotations for compactor service
    annotations: {}
  dnsConfigOverides:
    enabled: false
    dnsConfig:
      options:
        - name: ndots
          value: "3"    # This is required for Azure Kubernetes Service (AKS) https://github.com/grafana/tempo/issues/1462
  config:
    compaction:
      # -- Duration to keep blocks
      block_retention: 48h
      # Duration to keep blocks that have been compacted elsewhere
      compacted_block_retention: 1h
      # -- Blocks in this time window will be compacted together
      compaction_window: 1h
      # -- Amount of data to buffer from input blocks
      v2_in_buffer_bytes: 5242880
      # -- Flush data to backend when buffer is this large
      v2_out_buffer_bytes: 20971520
      # -- Maximum number of traces in a compacted block. WARNING: Deprecated. Use max_block_bytes instead.
      max_compaction_objects: 6000000
      # -- Maximum size of a compacted block in bytes
      max_block_bytes: 107374182400
      # -- Number of tenants to process in parallel during retention
      retention_concurrency: 10
      # -- Number of traces to buffer in memory during compaction
      v2_prefetch_snapshot_count: 1000
      # -- The maximum amount of time to spend compacting a single tenant before moving to the next
      max_time_per_tenant: 5m
      # -- The time between compaction cycles
      compaction_cycle: 30s

distributor:
  # -- Number of replicas for the distributor
  replicas: 1
  # -- hostAliases to add
  hostAliases: []
  #  - ip: 1.2.3.4
  #    hostnames:
  #      - domain.tld
  autoscaling:
    # -- Enable autoscaling for the distributor
    enabled: false
    # -- Minimum autoscaling replicas for the distributor
    minReplicas: 1
    # -- Maximum autoscaling replicas for the distributor
    maxReplicas: 3
    # -- Autoscaling behavior configuration for the distributor
    behavior: {}
    # -- Target CPU utilisation percentage for the distributor
    targetCPUUtilizationPercentage: 60
    # -- Target memory utilisation percentage for the distributor
    targetMemoryUtilizationPercentage:
  image:
    # -- The Docker registry for the ingester image. Overrides `global.image.registry`
    registry: null
    # -- Optional list of imagePullSecrets. Overrides `global.image.pullSecrets`
    pullSecrets: []
    # -- Docker image repository for the ingester image. Overrides `global.image.repository`
    repository: null
    # -- Docker image tag for the ingester image. Overrides `global.image.tag`
    tag: null
  service:
    # -- Annotations for distributor service
    annotations: {}
    # -- Labels for distributor service
    labels: {}
    # -- Type of service for the distributor
    type: ClusterIP
    # -- If type is LoadBalancer you can assign the IP to the LoadBalancer
    loadBalancerIP: ''
    # -- If type is LoadBalancer limit incoming traffic from IPs.
    loadBalancerSourceRanges: []
  deployment:
    annotations: {}
  serviceDiscovery:
    # -- Labels for distributorDiscovery service
    labels: {}
  # -- The name of the PriorityClass for distributor pods
  priorityClassName: null
  # -- Labels for distributor pods
  podLabels: {}
  # -- Annotations for distributor pods
  podAnnotations: {}
  # -- Additional CLI args for the distributor
  extraArgs: []
  # -- Environment variables to add to the distributor pods
  extraEnv: []
  # -- Environment variables from secrets or configmaps to add to the distributor pods
  extraEnvFrom: []
  # -- Resource requests and limits for the distributor
  resources: {}
  # -- Grace period to allow the distributor to shutdown before it is killed
  terminationGracePeriodSeconds: 30
  # -- topologySpread for distributor pods. Passed through `tpl` and, thus, to be configured as string
  # @default -- Defaults to allow skew no more then 1 node per AZ
  topologySpreadConstraints: |
    - maxSkew: 1
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: ScheduleAnyway
      labelSelector:
        matchLabels:
          {{- include "deep.selectorLabels" (dict "ctx" . "component" "distributor") | nindent 6 }}
  # -- Affinity for distributor pods. Passed through `tpl` and, thus, to be configured as string
  # @default -- Hard node and soft zone anti-affinity
  affinity: |
    podAntiAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchLabels:
              {{- include "deep.selectorLabels" (dict "ctx" . "component" "distributor") | nindent 10 }}
          topologyKey: kubernetes.io/hostname
      preferredDuringSchedulingIgnoredDuringExecution:
        - weight: 100
          podAffinityTerm:
            labelSelector:
              matchLabels:
                {{- include "deep.selectorLabels" (dict "ctx" . "component" "distributor") | nindent 12 }}
            topologyKey: topology.kubernetes.io/zone
  # -- Node selector for distributor pods
  nodeSelector: {}
  # -- Tolerations for distributor pods
  tolerations: []
  # -- Extra volumes for distributor pods
  extraVolumeMounts: []
  # -- Extra volumes for distributor deployment
  extraVolumes: []
  # -- Adds the appProtocol field to the distributor service. This allows distributor to work with istio protocol selection.
  appProtocol:
    # -- Set the optional grpc service protocol. Ex: "grpc", "http2" or "https"
    grpc: null
  # the config to be added to the deep.yaml config file
  config:
    log_received_snapshots:
      enabled: false
    receivers:
      deep:
        enabled: true

querier:
  # -- Number of replicas for the querier
  replicas: 1
  # -- hostAliases to add
  hostAliases: []
  #  - ip: 1.2.3.4
  #    hostnames:
  #      - domain.tld
  autoscaling:
    # -- Enable autoscaling for the querier
    enabled: false
    # -- Minimum autoscaling replicas for the querier
    minReplicas: 1
    # -- Maximum autoscaling replicas for the querier
    maxReplicas: 3
    # -- Autoscaling behavior configuration for the querier
    behavior: {}
    # -- Target CPU utilisation percentage for the querier
    targetCPUUtilizationPercentage: 60
    # -- Target memory utilisation percentage for the querier
    targetMemoryUtilizationPercentage:
  image:
    # -- The Docker registry for the querier image. Overrides `deep.image.registry`
    registry: null
    # -- Optional list of imagePullSecrets. Overrides `deep.image.pullSecrets`
    pullSecrets: []
    # -- Docker image repository for the querier image. Overrides `deep.image.repository`
    repository: null
    # -- Docker image tag for the querier image. Overrides `deep.image.tag`
    tag: null
  # -- The name of the PriorityClass for querier pods
  priorityClassName: null
  # -- Annotations for the querier deployment
  annotations: {}
  # -- Labels for querier pods
  podLabels: {}
  # -- Annotations for querier pods
  podAnnotations: {}
  # -- Additional CLI args for the querier
  extraArgs: []
  # -- Environment variables to add to the querier pods
  extraEnv: []
  # -- Environment variables from secrets or configmaps to add to the querier pods
  extraEnvFrom: []
  # -- Resource requests and limits for the querier
  resources: {}
  # -- Grace period to allow the querier to shutdown before it is killed
  terminationGracePeriodSeconds: 30
  # -- topologySpread for querier pods. Passed through `tpl` and, thus, to be configured as string
  # @default -- Defaults to allow skew no more then 1 node per AZ
  topologySpreadConstraints: |
    - maxSkew: 1
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: ScheduleAnyway
      labelSelector:
        matchLabels:
          {{- include "deep.selectorLabels" (dict "ctx" . "component" "querier") | nindent 6 }}
  # -- Affinity for querier pods. Passed through `tpl` and, thus, to be configured as string
  # @default -- Hard node and soft zone anti-affinity
  affinity: |
    podAntiAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchLabels:
              {{- include "deep.selectorLabels" (dict "ctx" . "component" "querier" "memberlist" true) | nindent 10 }}
          topologyKey: kubernetes.io/hostname
      preferredDuringSchedulingIgnoredDuringExecution:
        - weight: 100
          podAffinityTerm:
            labelSelector:
              matchLabels:
                {{- include "deep.selectorLabels" (dict "ctx" . "component" "querier" "memberlist" true) | nindent 12 }}
            topologyKey: topology.kubernetes.io/zone
  # -- Node selector for querier pods
  nodeSelector: {}
  # -- Tolerations for querier pods
  tolerations: []
  # -- Extra volumes for querier pods
  extraVolumeMounts: []
  # -- Extra volumes for querier deployment
  extraVolumes: []
  service:
    # -- Annotations for querier service
    annotations: {}
  # -- Adds the appProtocol field to the querier service. This allows querier to work with istio protocol selection.
  appProtocol:
    # -- Set the optional grpc service protocol. Ex: "grpc", "http2" or "https"
    grpc: null
  config:
    frontend_worker:
      frontend_address: '{{ include "deep.resourceName" (dict "ctx" . "component" "query-frontend-discovery") }}:9095'
    snapshot_by_id:
      # -- Timeout for trace lookup requests
      query_timeout: 10s
    search:
      # -- Timeout for search requests
      query_timeout: 30s
      # -- If search_external_endpoints is set then the querier will primarily act as a proxy for whatever serverless backend you have configured. This setting allows the operator to have the querier prefer itself for a configurable number of subqueries.
      prefer_self: 10
      # -- If set to a non-zero value a second request will be issued at the provided duration. Recommended to be set to p99 of external search requests to reduce long tail latency.
      external_hedge_requests_at: 8s
      # -- The maximum number of requests to execute when hedging. Requires hedge_requests_at to be set.
      external_hedge_requests_up_to: 2
      # -- A list of external endpoints that the querier will use to offload backend search requests
      external_endpoints: []
    # -- This value controls the overall number of simultaneous subqueries that the querier will service at once. It does not distinguish between the types of queries.
    max_concurrent_queries: 20

tracepoint:
  # -- Number of replicas for the querier
  replicas: 1
  # -- hostAliases to add
  hostAliases: []
  #  - ip: 1.2.3.4
  #    hostnames:
  #      - domain.tld
  autoscaling:
    # -- Enable autoscaling for the querier
    enabled: false
    # -- Minimum autoscaling replicas for the querier
    minReplicas: 1
    # -- Maximum autoscaling replicas for the querier
    maxReplicas: 3
    # -- Autoscaling behavior configuration for the querier
    behavior: {}
    # -- Target CPU utilisation percentage for the querier
    targetCPUUtilizationPercentage: 60
    # -- Target memory utilisation percentage for the querier
    targetMemoryUtilizationPercentage:
  image:
    # -- The Docker registry for the querier image. Overrides `deep.image.registry`
    registry: null
    # -- Optional list of imagePullSecrets. Overrides `deep.image.pullSecrets`
    pullSecrets: []
    # -- Docker image repository for the querier image. Overrides `deep.image.repository`
    repository: null
    # -- Docker image tag for the querier image. Overrides `deep.image.tag`
    tag: null
  # -- The name of the PriorityClass for querier pods
  priorityClassName: null
  # -- Annotations for the querier deployment
  annotations: {}
  # -- Labels for querier pods
  podLabels: {}
  # -- Annotations for querier pods
  podAnnotations: {}
  # -- Additional CLI args for the querier
  extraArgs: []
  # -- Environment variables to add to the querier pods
  extraEnv: []
  # -- Environment variables from secrets or configmaps to add to the querier pods
  extraEnvFrom: []
  # -- Resource requests and limits for the querier
  resources: {}
  # -- Grace period to allow the querier to shutdown before it is killed
  terminationGracePeriodSeconds: 30
  # -- topologySpread for querier pods. Passed through `tpl` and, thus, to be configured as string
  # @default -- Defaults to allow skew no more then 1 node per AZ
  topologySpreadConstraints: |
    - maxSkew: 1
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: ScheduleAnyway
      labelSelector:
        matchLabels:
          {{- include "deep.selectorLabels" (dict "ctx" . "component" "tracepoint") | nindent 6 }}
  # -- Affinity for querier pods. Passed through `tpl` and, thus, to be configured as string
  # @default -- Hard node and soft zone anti-affinity
  affinity: |
    podAntiAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchLabels:
              {{- include "deep.selectorLabels" (dict "ctx" . "component" "tracepoint" "memberlist" true) | nindent 10 }}
          topologyKey: kubernetes.io/hostname
      preferredDuringSchedulingIgnoredDuringExecution:
        - weight: 100
          podAffinityTerm:
            labelSelector:
              matchLabels:
                {{- include "deep.selectorLabels" (dict "ctx" . "component" "tracepoint" "memberlist" true) | nindent 12 }}
            topologyKey: topology.kubernetes.io/zone
  # -- Node selector for querier pods
  nodeSelector: {}
  # -- Tolerations for querier pods
  tolerations: []
  # -- Extra volumes for querier pods
  extraVolumeMounts: []
  # -- Extra volumes for querier deployment
  extraVolumes: []
  service:
    # -- Annotations for querier service
    annotations: {}
  # -- Adds the appProtocol field to the querier service. This allows querier to work with istio protocol selection.
  appProtocol:
    # -- Set the optional grpc service protocol. Ex: "grpc", "http2" or "https"
    grpc: null
  config:
    api:
      worker:
        frontend_address: '{{ include "deep.resourceName" (dict "ctx" . "component" "query-frontend-discovery") }}:9095'

tracepointApi:
  # -- Number of replicas for the tracepoint api
  replicas: 1
  # -- hostAliases to add
  hostAliases: []
  #  - ip: 1.2.3.4
  #    hostnames:
  #      - domain.tld
  autoscaling:
    # -- Enable autoscaling for the  tracepoint api
    enabled: false
    # -- Minimum autoscaling replicas for the  tracepoint api
    minReplicas: 1
    # -- Maximum autoscaling replicas for the  tracepoint api
    maxReplicas: 3
    # -- Autoscaling behavior configuration for the  tracepoint api
    behavior: {}
    # -- Target CPU utilisation percentage for the  tracepoint api
    targetCPUUtilizationPercentage: 60
    # -- Target memory utilisation percentage for the  tracepoint api
    targetMemoryUtilizationPercentage:
  image:
    # -- The Docker registry for the  tracepoint api image. Overrides `deep.image.registry`
    registry: null
    # -- Optional list of imagePullSecrets. Overrides `deep.image.pullSecrets`
    pullSecrets: []
    # -- Docker image repository for the  tracepoint api image. Overrides `deep.image.repository`
    repository: null
    # -- Docker image tag for the  tracepoint api image. Overrides `deep.image.tag`
    tag: null
  # -- The name of the PriorityClass for  tracepoint api pods
  priorityClassName: null
  # -- Annotations for the  tracepoint api deployment
  annotations: {}
  # -- Labels for  tracepoint api pods
  podLabels: {}
  # -- Annotations for  tracepoint api pods
  podAnnotations: {}
  # -- Additional CLI args for the  tracepoint api
  extraArgs: []
  # -- Environment variables to add to the  tracepoint api pods
  extraEnv: []
  # -- Environment variables from secrets or configmaps to add to the  tracepoint api pods
  extraEnvFrom: []
  # -- Resource requests and limits for the  tracepoint api
  resources: {}
  # -- Grace period to allow the  tracepoint api to shutdown before it is killed
  terminationGracePeriodSeconds: 30
  # -- topologySpread for  tracepoint api pods. Passed through `tpl` and, thus, to be configured as string
  # @default -- Defaults to allow skew no more then 1 node per AZ
  topologySpreadConstraints: |
    - maxSkew: 1
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: ScheduleAnyway
      labelSelector:
        matchLabels:
          {{- include "deep.selectorLabels" (dict "ctx" . "component" "tracepoint-api") | nindent 6 }}
  # -- Affinity for  tracepoint api pods. Passed through `tpl` and, thus, to be configured as string
  # @default -- Hard node and soft zone anti-affinity
  affinity: |
    podAntiAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchLabels:
              {{- include "deep.selectorLabels" (dict "ctx" . "component" "tracepoint-api" "memberlist" true) | nindent 10 }}
          topologyKey: kubernetes.io/hostname
      preferredDuringSchedulingIgnoredDuringExecution:
        - weight: 100
          podAffinityTerm:
            labelSelector:
              matchLabels:
                {{- include "deep.selectorLabels" (dict "ctx" . "component" "tracepoint-api" "memberlist" true) | nindent 12 }}
            topologyKey: topology.kubernetes.io/zone
  # -- Node selector for  tracepoint api pods
  nodeSelector: {}
  # -- Tolerations for  tracepoint api pods
  tolerations: []
  # -- Extra volumes for  tracepoint api pods
  extraVolumeMounts: []
  # -- Extra volumes for  tracepoint api deployment
  extraVolumes: []
  service:
    # -- Annotations for  tracepoint api service
    annotations: {}
  # -- Adds the appProtocol field to the  tracepoint api service. This allows  tracepoint api to work with istio protocol selection.
  appProtocol:
    # -- Set the optional grpc service protocol. Ex: "grpc", "http2" or "https"
    grpc: null


queryFrontend:
  # -- Number of replicas for the query-frontend
  replicas: 1
  # -- hostAliases to add
  hostAliases: []
  #  - ip: 1.2.3.4
  #    hostnames:
  #      - domain.tld
  # -- Annotations for the deployment
  annotations: {}
  autoscaling:
    # -- Enable autoscaling for the query-frontend
    enabled: false
    # -- Minimum autoscaling replicas for the query-frontend
    minReplicas: 1
    # -- Maximum autoscaling replicas for the query-frontend
    maxReplicas: 3
    # -- Autoscaling behavior configuration for the query-frontend
    behavior: {}
    # -- Target CPU utilisation percentage for the query-frontend
    targetCPUUtilizationPercentage: 60
    # -- Target memory utilisation percentage for the query-frontend
    targetMemoryUtilizationPercentage:
  image:
    # -- The Docker registry for the query-frontend image. Overrides `deep.image.registry`
    registry: null
    # -- Optional list of imagePullSecrets. Overrides `deep.image.pullSecrets`
    pullSecrets: []
    # -- Docker image repository for the query-frontend image. Overrides `deep.image.repository`
    repository: null
    # -- Docker image tag for the query-frontend image. Overrides `deep.image.tag`
    tag: null
  service:
    # -- Port of the query-frontend service
    port: 16686
    # -- Annotations for queryFrontend service
    annotations: {}
    # -- Labels for queryFrontend service
    labels: {}
    # -- Type of service for the queryFrontend
    type: ClusterIP
    # -- If type is LoadBalancer you can assign the IP to the LoadBalancer
    loadBalancerIP: ""
    # -- If type is LoadBalancer limit incoming traffic from IPs.
    loadBalancerSourceRanges: []
  serviceDiscovery:
    # -- Annotations for queryFrontendDiscovery service
    annotations: {}
    # -- Labels for queryFrontendDiscovery service
    labels: {}
  ingress:
    # -- Specifies whether an ingress for the Jaeger should be created
    enabled: false
    # -- Ingress Class Name. MAY be required for Kubernetes versions >= 1.18
    # ingressClassName: nginx
    # -- Annotations for the Jaeger ingress
    annotations: {}
    # -- Hosts configuration for the Jaeger ingress
    hosts:
      - host: query.deep.example.com
        paths:
          - path: /
            # -- pathType (e.g. ImplementationSpecific, Prefix, .. etc.) might also be required by some Ingress Controllers
            # pathType: Prefix
    # -- TLS configuration for the Jaeger ingress
    tls:
      - secretName: deep-query-tls
        hosts:
          - query.deep.example.com
  # -- The name of the PriorityClass for query-frontend pods
  priorityClassName: null
  # -- Labels for queryFrontend pods
  podLabels: {}
  # -- Annotations for query-frontend pods
  podAnnotations: {}
  # -- Additional CLI args for the query-frontend
  extraArgs: []
  # -- Environment variables to add to the query-frontend pods
  extraEnv: []
  # -- Environment variables from secrets or configmaps to add to the query-frontend pods
  extraEnvFrom: []
  # -- Resource requests and limits for the query-frontend
  resources: {}
  # -- Grace period to allow the query-frontend to shutdown before it is killed
  terminationGracePeriodSeconds: 30
  # -- topologySpread for query-frontend pods. Passed through `tpl` and, thus, to be configured as string
  # @default -- Defaults to allow skew no more then 1 node per AZ
  topologySpreadConstraints: |
    - maxSkew: 1
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: ScheduleAnyway
      labelSelector:
        matchLabels:
          {{- include "deep.selectorLabels" (dict "ctx" . "component" "query-frontend") | nindent 6 }}
  # -- Affinity for query-frontend pods. Passed through `tpl` and, thus, to be configured as string
  # @default -- Hard node and soft zone anti-affinity
  affinity: |
    podAntiAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchLabels:
              {{- include "deep.selectorLabels" (dict "ctx" . "component" "query-frontend") | nindent 10 }}
          topologyKey: kubernetes.io/hostname
      preferredDuringSchedulingIgnoredDuringExecution:
        - weight: 100
          podAffinityTerm:
            labelSelector:
              matchLabels:
                {{- include "deep.selectorLabels" (dict "ctx" . "component" "query-frontend") | nindent 12 }}
            topologyKey: topology.kubernetes.io/zone
  # -- Node selector for query-frontend pods
  nodeSelector: {}
  # -- Tolerations for query-frontend pods
  tolerations: []
  # -- Extra volumes for query-frontend pods
  extraVolumeMounts: []
  # -- Extra volumes for query-frontend deployment
  extraVolumes: []
  # -- Adds the appProtocol field to the queryFrontend service. This allows queryFrontend to work with istio protocol selection.
  appProtocol:
    # -- Set the optional grpc service protocol. Ex: "grpc", "http2" or "https"
    grpc: null
  config:
    # -- Number of times to retry a request sent to a querier
    max_retries: 2
    search:
      # -- The number of concurrent jobs to execute when searching the backend
      concurrent_jobs: 1000
      # -- The target number of bytes for each job to handle when performing a backend search
      target_bytes_per_job: 104857600
    # -- Snapshot by ID lookup configuration
    snapshot_by_id:
      # -- The number of shards to split a snapshot by id query into.
      query_shards: 50
      # -- If set to a non-zero value, a second request will be issued at the provided duration. Recommended to be set to p99 of search requests to reduce long-tail latency.
      hedge_requests_at: 2s
      # -- The maximum number of requests to execute when hedging. Requires hedge_requests_at to be set. Must be greater than 0.
      hedge_requests_up_to: 2

ingester:
  # -- Annotations for the ingester StatefulSet
  annotations: {}
  # -- Number of replicas for the ingester
  replicas: 3
  # -- hostAliases to add
  hostAliases: []
  #  - ip: 1.2.3.4
  #    hostnames:
  #      - domain.tld
  autoscaling:
    # -- Enable autoscaling for the ingester. WARNING: Autoscaling ingesters can result in lost data. Only do this if you know what you're doing.
    enabled: false
    # -- Minimum autoscaling replicas for the ingester
    minReplicas: 1
    # -- Maximum autoscaling replicas for the ingester
    maxReplicas: 3
    # -- Autoscaling behavior configuration for the ingester
    behavior: {}
    # -- Target CPU utilisation percentage for the ingester
    targetCPUUtilizationPercentage: 60
    # -- Target memory utilisation percentage for the ingester
    targetMemoryUtilizationPercentage:
  image:
    # -- The Docker registry for the ingester image. Overrides `global.image.registry`
    registry: null
    # -- Optional list of imagePullSecrets. Overrides `global.image.pullSecrets`
    pullSecrets: []
    # -- Docker image repository for the ingester image. Overrides `global.image.repository`
    repository: null
    # -- Docker image tag for the ingester image. Overrides `global.image.tag`
    tag: null
  # -- The name of the PriorityClass for ingester pods
  priorityClassName: null
  # -- Labels for ingester pods
  podLabels: {}
  # -- Annotations for ingester pods
  podAnnotations: {}
  # -- Additional CLI args for the ingester
  extraArgs: []
  # -- Environment variables to add to the ingester pods
  extraEnv: []
  # -- Environment variables from secrets or configmaps to add to the ingester pods
  extraEnvFrom: []
  # -- Resource requests and limits for the ingester
  resources: {}
  # -- Grace period to allow the ingester to shutdown before it is killed. Especially for the ingestor,
  # this must be increased. It must be long enough so ingesters can be gracefully shutdown flushing/transferring
  # all data and to successfully leave the member ring on shutdown.
  terminationGracePeriodSeconds: 300
  # -- topologySpread for ingester pods. Passed through `tpl` and, thus, to be configured as string
  # @default -- Defaults to allow skew no more then 1 node per AZ
  topologySpreadConstraints: |
    - maxSkew: 1
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: ScheduleAnyway
      labelSelector:
        matchLabels:
          {{- include "deep.selectorLabels" (dict "ctx" . "component" "ingester") | nindent 6 }}
  # -- Affinity for ingester pods. Passed through `tpl` and, thus, to be configured as string
  # @default -- Soft node and soft zone anti-affinity
  affinity: |
    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
        - weight: 100
          podAffinityTerm:
            labelSelector:
              matchLabels:
                {{- include "deep.selectorLabels" (dict "ctx" . "component" "ingester") | nindent 12 }}
            topologyKey: kubernetes.io/hostname
        - weight: 75
          podAffinityTerm:
            labelSelector:
              matchLabels:
                {{- include "deep.selectorLabels" (dict "ctx" . "component" "ingester") | nindent 12 }}
            topologyKey: topology.kubernetes.io/zone
  # -- Node selector for ingester pods
  nodeSelector: {}
  # -- Tolerations for ingester pods
  tolerations: []
  # -- Extra volumes for ingester pods
  extraVolumeMounts: []
  # -- Extra volumes for ingester deployment
  extraVolumes: []
  persistence:
    # -- Enable creating PVCs which is required when using boltdb-shipper
    enabled: false
    # -- use emptyDir with ramdisk instead of PVC. **Please note that all data in ingester will be lost on pod restart**
    inMemory: false
    # -- Size of persistent or memory disk
    size: 10Gi
    # -- Storage class to be used.
    # If defined, storageClassName: <storageClass>.
    # If set to "-", storageClassName: "", which disables dynamic provisioning.
    # If empty or set to null, no storageClassName spec is
    # set, choosing the default provisioner (gp2 on AWS, standard on GKE, AWS, and OpenStack).
    storageClass: null
    # -- Annotations for ingester's persist volume claim
    annotations: {}
  service:
    # -- Annotations for ingester service
    annotations: {}
  # -- Adds the appProtocol field to the ingester service. This allows ingester to work with istio protocol selection.
  appProtocol:
    # -- Set the optional grpc service protocol. Ex: "grpc", "http2" or "https"
    grpc: null
  config:
    lifecycler:
      ring:
        # -- Number of copies of snapshots to store in the ingester ring
        replication_factor: 3
        kvstore:
          store: memberlist
      tokens_file_path: /var/deep/tokens.json

# To configure a different storage backend instead of local storage:
# storage:
#   tracepoint:
#     backend: s3
#     s3:
#       bucket: <bucketname>
#       access_key: <token>
#       secret_key: <secret>
#       region: eu-west-1
#       endpoint: s3.dualstack.eu-west-1.amazonaws.com
storage:
  tracepoint:
    # -- The supported storage backends are gcs, s3 and azure
    backend: local
    wal:
      path: /var/deep/wal
    local:
      path: /var/deep/blocks

metricsGenerator:
  enabled: false

memberlist:
  node_name: ""
  randomize_node_name: true
  stream_timeout: "10s"
  retransmit_factor: 2
  pull_push_interval: "30s"
  gossip_interval: "1s"
  gossip_nodes: 2
  gossip_to_dead_nodes_time: "30s"
  min_join_backoff: "1s"
  max_join_backoff: "1m"
  max_join_retries: 10
  abort_if_cluster_join_fails: false
  rejoin_interval: "0s"
  left_ingesters_timeout: "5m"
  leave_timeout: "5s"
  bind_addr: []
  bind_port: 7946
  packet_dial_timeout: "5s"
  packet_write_timeout: "5s"

# Global overrides
global_overrides:
  per_tenant_override_config: /runtime-config/overrides.yaml
  metrics_generator_processors: []

# Per tenants overrides
overrides:
  # -- Configuration is loaded from the secret called 'externalConfigSecretName'.
  # If 'useExternalConfig' is true, then the configuration is not generated, just
  # consumed.
  useExternalConfig: false
  # -- Name of the Secret or ConfigMap that contains the runtime configuration (used for naming even if config is internal).
  externalRuntimeConfigName: '{{ include "deep.resourceName" (dict "ctx" . "component" "runtime") }}'
  overrides: |
    overrides: {}

# Set Deep server configuration
server:
  # --  HTTP server listen host
  httpListenPort: 3100
  # -- Log level. Can be set to trace, debug, info (default), warn, error, fatal, panic
  logLevel: info
  # -- Log format. Can be set to logfmt (default) or json.
  logFormat: logfmt
  # -- Max gRPC message size that can be received
  grpc_server_max_recv_msg_size: 4194304
  # -- Max gRPC message size that can be sent
  grpc_server_max_send_msg_size: 4194304
  # -- Read timeout for HTTP server
  http_server_read_timeout: 30s
  # -- Write timeout for HTTP server
  http_server_write_timeout: 30s

# define the config for deep
config:
  # -- Configuration is loaded from the secret called 'externalConfigSecretName'.
  # If 'useExternalConfig' is true, then the configuration is not generated, just
  # consumed.
  useExternalConfig: false

  # The name of the cluster, can be used to identify the cluster if running more than one
  clusterName: null

  # -- Defines what kind of object stores the configuration, a ConfigMap or a Secret.
  # In order to move sensitive information (such as credentials) from the ConfigMap/Secret to a more secure location (e.g. vault), it is possible to use [environment variables in the configuration](https://grafana.com/docs/mimir/latest/operators-guide/configuring/reference-configuration-parameters/#use-environment-variables-in-the-configuration).
  # Such environment variables can be then stored in a separate Secret and injected via the global.extraEnvFrom value. For details about environment injection from a Secret please see [Secrets](https://kubernetes.io/docs/concepts/configuration/secret/#use-case-as-container-environment-variables).
  storageType: ConfigMap

  # -- Name of the Secret or ConfigMap that contains the configuration (used for naming even if config is internal).
  externalSecretName: '{{ include "deep.resourceName" (dict "ctx" . "component" "config") }}'

  multitenancyEnabled: true

  # -- Structured deep configuration
  structuredConfig: {}

  deep: |
    multitenancy_enabled: {{ .Values.config.multitenancyEnabled }}

    compactor:
      {{- if .Values.compactor.config }}
        {{- toYaml .Values.compactor.config | nindent 2 }}
      {{- end }}
      ring:
        kvstore:
          store: memberlist

    distributor:
      ring:
        kvstore:
          store: memberlist
      {{- if .Values.distributor.config }}
        {{- toYaml .Values.distributor.config | nindent 2 }}
      {{- end }}

    querier:
      {{- if .Values.querier.config }}
        {{- toYaml .Values.querier.config | nindent 2 }}
      {{- end }}

    query_frontend:
      {{- if .Values.queryFrontend.config }}
        {{- toYaml .Values.queryFrontend.config | nindent 2 }}
      {{- end }}

    ingester:
      {{- if .Values.ingester.config }}
        {{- toYaml .Values.ingester.config | nindent 2 }}
      {{- end }}

    memberlist:
      {{- with .Values.memberlist }}
        {{- toYaml . | nindent 2 }}
      {{- end }}
      join_members:
        - dns+{{ include "deep.fullname" . }}-gossip-ring:{{ .Values.memberlist.bind_port }}

    tracepoint:
      {{- with .Values.tracepoint.config }}
        {{- toYaml . | nindent 2 }}
      {{- end }}

    overrides:
      {{- toYaml .Values.global_overrides | nindent 2 }}
      {{- if .Values.metricsGenerator.enabled }}
      metrics_generator_processors:
      {{- range .Values.global_overrides.metrics_generator_processors }}
      - {{ . }}
      {{- end }}
      {{- end }}

    server:
      http_listen_port: {{ .Values.server.httpListenPort }}
      log_level: {{ .Values.server.logLevel }}
      log_format: {{ .Values.server.logFormat }}
      grpc_server_max_recv_msg_size: {{ .Values.server.grpc_server_max_recv_msg_size }}
      grpc_server_max_send_msg_size: {{ .Values.server.grpc_server_max_send_msg_size }}
      http_server_read_timeout: {{ .Values.server.http_server_read_timeout }}
      http_server_write_timeout: {{ .Values.server.http_server_write_timeout }}

    storage:
      {{- if .Values.storage }}
        {{- toYaml .Values.storage | nindent 2 }}
      {{- end }}

serviceAccount:
  # -- Specifies whether a ServiceAccount should be created
  create: true
  # -- The name of the ServiceAccount to use.
  # If not set and create is true, a name is generated using the fullname template
  name: null
  # -- Image pull secrets for the service account
  imagePullSecrets: []
  # -- Annotations for the service account
  annotations: {}

## This config controls the deployment of Pod/Service Monitors to connect Deep to Prometheus for metric gathering
monitoring:
  # Is monitoring active, set to true to deploy the service monitor
  enabled: false
  # ServiceMonitor configuration
  serviceMonitor:
    # the label to use to identify this cluster
    clusterLabel: "cluster"
    # -- Alternative namespace for ServiceMonitor resources
    namespace: null
    # -- Namespace selector for ServiceMonitor resources
    namespaceSelector: {}
    # -- ServiceMonitor annotations
    annotations: {}
    # -- Additional ServiceMonitor labels
    labels: {}
    # -- ServiceMonitor scrape interval
    interval: null
    # -- ServiceMonitor scrape timeout in Go duration format (e.g. 15s)
    scrapeTimeout: null
    # -- ServiceMonitor relabel configs to apply to samples before scraping
    # https://github.com/prometheus-operator/prometheus-operator/blob/master/Documentation/api.md#relabelconfig
    relabelings: []
    # -- ServiceMonitor metric relabel configs to apply to samples before ingestion
    # https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#endpoint
    metricRelabelings: []
    # -- ServiceMonitor will use http by default, but you can pick https as well
    scheme: http
    # -- ServiceMonitor will use these tlsConfig settings to make the health check requests
    tlsConfig: null
  # This controls the config for provided dashboards
  dashboards:
    # if enabled the dashboards will be provided
    enabled: false
    # The namespace to deploy the dashboards in
    namespace: null
    # Additional annotations to add to the dashboards
    annotations: {}
    # additional labels to add to the dashboards
    labels:
      grafana_dashboard: "1"
  # This controls the prometheus rules
  rules:
    # if enabled the prometheus rules will be deployed
    enabled: false
    # additional labels to add to the PrometheusRule
    labels:
      release: prometheus
    # Additional annotations to add to the PrometheusRule
    annotations: {}
    # The namespace to deploy the rules in
    namespace: null
    # Additional labels to add to the alerts
    alertLabels:
      app_name: '{{ include "deep.fullname" . }}'
    # Additional annotations to add to the alerts
    alertAnnotations: {}
    # The rules to add
    rules:
      - alert: Unhealthy Ring Member
        expr: deep_ring_members{state="Unhealthy"} > 0
        labels:
          severity: critical
        annotations:
          description: "{{ $labels.service }} has {{ $value }} unhealthy nodes in ring {{ $labels.name }}."
          runbook_url: https://intergral.github.io/deep/runbooks/unhealthy_ring_node/
          summary: One or more ring members are unhealthy
      - alert: Required Ring Member Missing
        expr: deep_ring_members{state="ACTIVE", cluster="{{ include "deep.clusterName" .}}", name=~"ingester|tracepoint"} == 0
        labels:
          severity: critical
        annotations:
          description: "{{ $labels.service }} has {{ $value }} active nodes in ring {{ $labels.name }}."
          runbook_url: https://intergral.github.io/deep/runbooks/missing_ring_node/
          summary: One or more ring has missing required nodes
      - alert: Tracepoint ring missing member
        expr: deep_ring_members{state="ACTIVE", cluster="{{ include "deep.clusterName" .}}", name="tracepoint"} != {{ .Values.tracepoint.replicas }}
        labels:
          severity: critical
        annotations:
          description: "{{ $labels.service }} has {{ $value }} active nodes in ring {{ $labels.name }}."
          runbook_url: https://intergral.github.io/deep/runbooks/missing_ring_node/
          summary: One or more ring has missing required nodes
      - alert: Ingester ring missing member
        expr: deep_ring_members{state="ACTIVE", cluster="{{ include "deep.clusterName" .}}", name="ingester"} != {{ .Values.ingester.replicas }}
        labels:
          severity: critical
        annotations:
          description: "{{ $labels.service }} has {{ $value }} active nodes in ring {{ $labels.name }}."
          runbook_url: https://intergral.github.io/deep/runbooks/missing_ring_node/
          summary: One or more ring has missing required nodes
      - alert: Compactor ring missing member
        expr: deep_ring_members{state="ACTIVE", cluster="{{ include "deep.clusterName" .}}", name="compactor"} != {{ .Values.compactor.replicas }}
        labels:
          severity: critical
        annotations:
          description: "{{ $labels.service }} has {{ $value }} active nodes in ring {{ $labels.name }}."
          runbook_url: https://intergral.github.io/deep/runbooks/missing_ring_node/
          summary: One or more ring has missing required nodes
      - alert: Metrics Generator ring missing member
        expr: deep_ring_members{state="ACTIVE", cluster="{{ include "deep.clusterName" .}}", name="metrics-generator"} != {{ if .Values.metricsGenerator.enabled }} {{.Values.distributor.replicas }} {{else}}0{{end}}
        labels:
          severity: critical
        annotations:
          description: "{{ $labels.service }} has {{ $value }} active nodes in ring {{ $labels.name }}."
          runbook_url: https://intergral.github.io/deep/runbooks/missing_ring_node/
          summary: One or more ring has missing required nodes
      - alert: Rapid increase in blocks
        expr: delta(deep_db_blocklist_length{cluster="{{ include "deep.clusterName" .}}"}[24h]) > 30
        for: 48h
        labels:
          severity: warning
        annotations:
          description: "Tenant: {{ $labels.tenant }} has seen a large increase in blocks: {{ $value }}."
          runbook_url: https://intergral.github.io/deep/runbooks/block_increase/
          summary: Tenant has seen large increase in blocks.


# Configuration for the gateway
gateway:
  # -- Specifies whether the gateway should be enabled
  enabled: false
  # -- Number of replicas for the gateway
  replicas: 1
  # -- hostAliases to add
  hostAliases: []
  #  - ip: 1.2.3.4
  #    hostnames:
  #      - domain.tld
  autoscaling:
    # -- Enable autoscaling for the gateway
    enabled: false
    # -- Minimum autoscaling replicas for the gateway
    minReplicas: 1
    # -- Maximum autoscaling replicas for the gateway
    maxReplicas: 3
    # -- Autoscaling behavior configuration for the gateway
    behavior: {}
    # -- Target CPU utilisation percentage for the gateway
    targetCPUUtilizationPercentage: 60
    # -- Target memory utilisation percentage for the gateway
    targetMemoryUtilizationPercentage:
  # -- Enable logging of 2xx and 3xx HTTP requests
  verboseLogging: true
  image:
    # -- The Docker registry for the gateway image. Overrides `global.image.registry`
    registry: null
    # -- Optional list of imagePullSecrets. Overrides `global.image.pullSecrets`
    pullSecrets: []
    # -- The gateway image repository
    repository: nginxinc/nginx-unprivileged
    # -- The gateway image tag
    tag: 1.19-alpine
    # -- The gateway image pull policy
    pullPolicy: IfNotPresent
  # -- The name of the PriorityClass for gateway pods
  priorityClassName: null
  # -- Labels for gateway pods
  podLabels: {}
  # -- Annotations for gateway pods
  podAnnotations: {}
  # -- Additional CLI args for the gateway
  extraArgs: []
  # -- Environment variables to add to the gateway pods
  extraEnv: []
  # -- Environment variables from secrets or configmaps to add to the gateway pods
  extraEnvFrom: []
  # -- Volumes to add to the gateway pods
  extraVolumes: []
  # -- Volume mounts to add to the gateway pods
  extraVolumeMounts: []
  # -- Resource requests and limits for the gateway
  resources: {}
  # -- Grace period to allow the gateway to shutdown before it is killed
  terminationGracePeriodSeconds: 30
  # -- topologySpread for gateway pods. Passed through `tpl` and, thus, to be configured as string
  # @default -- Defaults to allow skew no more then 1 node per AZ
  topologySpreadConstraints: |
    - maxSkew: 1
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: ScheduleAnyway
      labelSelector:
        matchLabels:
          {{- include "deep.selectorLabels" (dict "ctx" . "component" "gateway") | nindent 6 }}
  # -- Affinity for gateway pods. Passed through `tpl` and, thus, to be configured as string
  # @default -- Hard node and soft zone anti-affinity
  affinity: |
    podAntiAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchLabels:
              {{- include "deep.selectorLabels" (dict "ctx" . "component" "gateway") | nindent 10 }}
          topologyKey: kubernetes.io/hostname
      preferredDuringSchedulingIgnoredDuringExecution:
        - weight: 100
          podAffinityTerm:
            labelSelector:
              matchLabels:
                {{- include "deep.selectorLabels" (dict "ctx" . "component" "gateway") | nindent 12 }}
            topologyKey: topology.kubernetes.io/zone
  # -- Node selector for gateway pods
  nodeSelector: {}
  # -- Tolerations for gateway pods
  tolerations: []
  # Gateway service configuration
  service:
    # -- Port of the gateway service
    port: 80
    # -- Type of the gateway service
    type: ClusterIP
    # -- ClusterIP of the gateway service
    clusterIP: null
    # -- Node port if service type is NodePort
    nodePort: null
    # -- Load balancer IPO address if service type is LoadBalancer
    loadBalancerIP: null
    # -- Annotations for the gateway service
    annotations: {}
    # -- Labels for gateway service
    labels: {}
    # -- Additional ports to be opneed on gateway service (e.g. for RPC connections)
    additionalPorts: {}
  # config for using traefik 'IngressRoute'
  traefik:
    # -- Specified whether an IngressRoute should be created
    enabled: false
    host: gateway.deep.example.com
    # -- Specify the entry points for traefik
    entryPoints:
      - websecure
    routes:
      - match: Host(`{{ .Values.gateway.traefik.host }}`) && PathPrefix(`/deepproto.proto`)
        kind: Rule
        services:
          - name: '{{ include "deep.fullname" . }}-distributor'
            port: 43315
            scheme: h2c
      - match: Host(`{{ .Values.gateway.traefik.host }}`)
        kind: Rule
        services:
          - name: '{{ include "deep.fullname" . }}-gateway'
            port: 80
  # Gateway ingress configuration
  ingress:
    # -- Specifies whether an ingress for the gateway should be created
    enabled: false
    # -- Ingress Class Name. MAY be required for Kubernetes versions >= 1.18
    # ingressClassName: nginx
    # -- Annotations for the gateway ingress
    annotations: {}
    # -- Hosts configuration for the gateway ingress
    hosts:
      - host: gateway.deep.example.com
        paths:
          - path: /
            # -- pathType (e.g. ImplementationSpecific, Prefix, .. etc.) might also be required by some Ingress Controllers
            pathType: Prefix
    # -- TLS configuration for the gateway ingress
    tls:
      - secretName: deep-gateway-tls
        hosts:
          - gateway.deep.example.com
  # Basic auth configuration
  basicAuth:
    # -- Enables basic authentication for the gateway
    enabled: false
    # -- The basic auth username for the gateway
    username: null
    # -- The basic auth password for the gateway
    password: null
    # -- Uses the specified username and password to compute a htpasswd using Sprig's `htpasswd` function.
    # The value is templated using `tpl`. Override this to use a custom htpasswd, e.g. in case the default causes
    # high CPU load.
    htpasswd: >-
      {{ htpasswd (required "'gateway.basicAuth.username' is required" .Values.gateway.basicAuth.username) (required "'gateway.basicAuth.password' is required" .Values.gateway.basicAuth.password) }}
    # -- Existing basic auth secret to use. Must contain '.htpasswd'
    existingSecret: null
  # Configures the readiness probe for the gateway
  readinessProbe:
    httpGet:
      path: /
      port: http-metrics
    initialDelaySeconds: 15
    timeoutSeconds: 1
  nginxConfig:
    includeStatus: false
    statusConfig: |-
      location ^~ /status {
        proxy_pass http://{{ include "deep.resourceName" (dict "ctx" . "component" "query-frontend") }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:3100$request_uri;
      }
    # -- NGINX log format
    logFormat: |-
      main '$remote_addr - $remote_user [$time_local]  $status '
              '"$request" $body_bytes_sent "$http_referer" '
              '"$http_user_agent" "$http_x_forwarded_for"';
    # -- Allows appending custom configuration to the server block
    serverSnippet: ''
    # -- Allows appending custom configuration to the http block
    httpSnippet: ''
    # -- Allows overriding the DNS resolver address nginx will use
    resolver: ''
    # -- Config file contents for Nginx. Passed through the `tpl` function to allow templating
    # @default -- See values.yaml
    file: |
      worker_processes  5;  ## Default: 1
      error_log  /dev/stderr;
      pid        /tmp/nginx.pid;
      worker_rlimit_nofile 8192;

      events {
        worker_connections  4096;  ## Default: 1024
      }

      http {
        client_body_temp_path /tmp/client_temp;
        proxy_temp_path       /tmp/proxy_temp_path;
        fastcgi_temp_path     /tmp/fastcgi_temp;
        uwsgi_temp_path       /tmp/uwsgi_temp;
        scgi_temp_path        /tmp/scgi_temp;

        proxy_http_version    1.1;

        default_type application/octet-stream;
        log_format   {{ .Values.gateway.nginxConfig.logFormat }}

        {{- if .Values.gateway.verboseLogging }}
        access_log   /dev/stderr  main;
        {{- else }}

        map $status $loggable {
          ~^[23]  0;
          default 1;
        }
        access_log   /dev/stderr  main  if=$loggable;
        {{- end }}

        sendfile     on;
        tcp_nopush   on;
        {{- if .Values.gateway.nginxConfig.resolver }}
        resolver {{ .Values.gateway.nginxConfig.resolver }};
        {{- else }}
        resolver {{ .Values.global.dnsService }}.{{ .Values.global.dnsNamespace }}.svc.{{ .Values.global.clusterDomain }};
        {{- end }}

        {{- with .Values.gateway.nginxConfig.httpSnippet }}
        {{ . | nindent 2 }}
        {{- end }}

        server {
          listen             8080;

          {{- if .Values.gateway.basicAuth.enabled }}
          auth_basic           "deep";
          auth_basic_user_file /etc/nginx/secrets/.htpasswd;
          {{- end }}

          location = / {
            return 200 'OK';
            auth_basic off;
          }

          location ^~ /api {
            proxy_pass       http://{{ include "deep.resourceName" (dict "ctx" . "component" "query-frontend") }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:3100$request_uri;
          }

          location = /flush {
            proxy_pass       http://{{ include "deep.resourceName" (dict "ctx" . "component" "ingester") }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:3100$request_uri;
          }

          location = /shutdown {
            proxy_pass       http://{{ include "deep.resourceName" (dict "ctx" . "component" "ingester") }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:3100$request_uri;
          }

          location = /distributor/ring {
            proxy_pass       http://{{ include "deep.resourceName" (dict "ctx" . "component" "distributor") }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:3100$request_uri;
          }

          location = /ingester/ring {
            proxy_pass       http://{{ include "deep.resourceName" (dict "ctx" . "component" "distributor") }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:3100$request_uri;
          }

          location = /compactor/ring {
            proxy_pass       http://{{ include "deep.resourceName" (dict "ctx" . "component" "compactor") }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:3100$request_uri;
          }

          location = /tracepoint/ring {
            proxy_pass       http://{{ include "deep.resourceName" (dict "ctx" . "component" "tracepoint-api") }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:3100$request_uri;
          }

          {{- if .Values.gateway.nginxConfig.includeStatus }}
          {{ tpl .Values.gateway.nginxConfig.statusConfig . | nindent 4 }}
          {{- end }}

          {{- with .Values.gateway.nginxConfig.serverSnippet }}
          {{ . | nindent 4 }}
          {{- end }}
        }
      }
