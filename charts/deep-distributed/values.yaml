# Default values for deep.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.
global:
  image:
    # define the default image config for all containers
    registry: docker.io
    repository: intergral/deep
    tag: null
    pullSecrets: []
    pullPolicy: IfNotPresent
  # -- Overrides the priorityClassName for all pods
  priorityClassName: null

nameOverride: ""
fullnameOverride: ""
namespace: "deep"

deep:
  # -- Global labels for all tempo pods
  podLabels: { }
  # -- Common annotations for all pods
  podAnnotations: {}
  # -- podSecurityContext holds pod-level security attributes and common container settings
  podSecurityContext:
    fsGroup: 1000
  readinessProbe:
    httpGet:
      path: /ready
      port: http-metrics
    initialDelaySeconds: 30
    timeoutSeconds: 1
  # -- SecurityContext holds container-level security attributes and common container settings
  securityContext:
    runAsNonRoot: true
    runAsUser: 1000
    runAsGroup: 1000
    allowPrivilegeEscalation: false
    capabilities:
      drop:
        - ALL
    readOnlyRootFilesystem: true

compactor:
  config:
    compaction:
      # -- Duration to keep blocks
      block_retention: 48h
      # Duration to keep blocks that have been compacted elsewhere
      compacted_block_retention: 1h
      # -- Blocks in this time window will be compacted together
      compaction_window: 1h
      # -- Amount of data to buffer from input blocks
      v2_in_buffer_bytes: 5242880
      # -- Flush data to backend when buffer is this large
      v2_out_buffer_bytes: 20971520
      # -- Maximum number of traces in a compacted block. WARNING: Deprecated. Use max_block_bytes instead.
      max_compaction_objects: 6000000
      # -- Maximum size of a compacted block in bytes
      max_block_bytes: 107374182400
      # -- Number of tenants to process in parallel during retention
      retention_concurrency: 10
      # -- Number of traces to buffer in memory during compaction
      v2_prefetch_snapshot_count: 1000
      # -- The maximum amount of time to spend compacting a single tenant before moving to the next
      max_time_per_tenant: 5m
      # -- The time between compaction cycles
      compaction_cycle: 30s

distributor:
  # -- Number of replicas for the distributor
  replicas: 1
  # -- hostAliases to add
  hostAliases: [ ]
  #  - ip: 1.2.3.4
  #    hostnames:
  #      - domain.tld
  autoscaling:
    # -- Enable autoscaling for the distributor
    enabled: false
    # -- Minimum autoscaling replicas for the distributor
    minReplicas: 1
    # -- Maximum autoscaling replicas for the distributor
    maxReplicas: 3
    # -- Autoscaling behavior configuration for the distributor
    behavior: { }
    # -- Target CPU utilisation percentage for the distributor
    targetCPUUtilizationPercentage: 60
    # -- Target memory utilisation percentage for the distributor
    targetMemoryUtilizationPercentage:
  image:
    # -- The Docker registry for the ingester image. Overrides `global.image.registry`
    registry: null
    # -- Optional list of imagePullSecrets. Overrides `global.image.pullSecrets`
    pullSecrets: [ ]
    # -- Docker image repository for the ingester image. Overrides `global.image.repository`
    repository: null
    # -- Docker image tag for the ingester image. Overrides `global.image.tag`
    tag: null
  service:
    # -- Annotations for distributor service
    annotations: {}
    # -- Labels for distributor service
    labels: {}
    # -- Type of service for the distributor
    type: ClusterIP
    # -- If type is LoadBalancer you can assign the IP to the LoadBalancer
    loadBalancerIP: ''
    # -- If type is LoadBalancer limit incoming traffic from IPs.
    loadBalancerSourceRanges: [ ]
  deployment:
    annotations: {}
  serviceDiscovery:
    # -- Labels for distributorDiscovery service
    labels: { }
  # -- The name of the PriorityClass for distributor pods
  priorityClassName: null
  # -- Labels for distributor pods
  podLabels: { }
  # -- Annotations for distributor pods
  podAnnotations: { }
  # -- Additional CLI args for the distributor
  extraArgs: [ ]
  # -- Environment variables to add to the distributor pods
  extraEnv: [ ]
  # -- Environment variables from secrets or configmaps to add to the distributor pods
  extraEnvFrom: [ ]
  # -- Resource requests and limits for the distributor
  resources: { }
  # -- Grace period to allow the distributor to shutdown before it is killed
  terminationGracePeriodSeconds: 30
  # -- topologySpread for distributor pods. Passed through `tpl` and, thus, to be configured as string
  # @default -- Defaults to allow skew no more then 1 node per AZ
  topologySpreadConstraints: |
    - maxSkew: 1
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: ScheduleAnyway
      labelSelector:
        matchLabels:
          {{- include "deep.selectorLabels" (dict "ctx" . "component" "distributor") | nindent 6 }}
  # -- Affinity for distributor pods. Passed through `tpl` and, thus, to be configured as string
  # @default -- Hard node and soft zone anti-affinity
  affinity: |
    podAntiAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchLabels:
              {{- include "deep.selectorLabels" (dict "ctx" . "component" "distributor") | nindent 10 }}
          topologyKey: kubernetes.io/hostname
      preferredDuringSchedulingIgnoredDuringExecution:
        - weight: 100
          podAffinityTerm:
            labelSelector:
              matchLabels:
                {{- include "deep.selectorLabels" (dict "ctx" . "component" "distributor") | nindent 12 }}
            topologyKey: topology.kubernetes.io/zone
  # -- Node selector for distributor pods
  nodeSelector: { }
  # -- Tolerations for distributor pods
  tolerations: [ ]
  # -- Extra volumes for distributor pods
  extraVolumeMounts: [ ]
  # -- Extra volumes for distributor deployment
  extraVolumes: [ ]
  config:
    log_received_snapshots:
      enabled: false
    receivers:
      deep:
        enabled: true

querier:
  config:
    frontend_worker:
      frontend_address: '{{ include "deep.resourceName" (dict "ctx" . "component" "query-frontend-discovery") }}:9095'
    snapshot_by_id:
      # -- Timeout for trace lookup requests
      query_timeout: 10s
    search:
      # -- Timeout for search requests
      query_timeout: 30s
      # -- If search_external_endpoints is set then the querier will primarily act as a proxy for whatever serverless backend you have configured. This setting allows the operator to have the querier prefer itself for a configurable number of subqueries.
      prefer_self: 10
      # -- If set to a non-zero value a second request will be issued at the provided duration. Recommended to be set to p99 of external search requests to reduce long tail latency.
      external_hedge_requests_at: 8s
      # -- The maximum number of requests to execute when hedging. Requires hedge_requests_at to be set.
      external_hedge_requests_up_to: 2
      # -- A list of external endpoints that the querier will use to offload backend search requests
      external_endpoints: [ ]

    # -- This value controls the overall number of simultaneous subqueries that the querier will service at once. It does not distinguish between the types of queries.
    max_concurrent_queries: 20

queryFrontend:
  config:
    # -- Number of times to retry a request sent to a querier
    max_retries: 2
    search:
      # -- The number of concurrent jobs to execute when searching the backend
      concurrent_jobs: 1000
      # -- The target number of bytes for each job to handle when performing a backend search
      target_bytes_per_job: 104857600
    # -- Snapshot by ID lookup configuration
    snapshot_by_id:
      # -- The number of shards to split a snapshot by id query into.
      query_shards: 50
      # -- If set to a non-zero value, a second request will be issued at the provided duration. Recommended to be set to p99 of search requests to reduce long-tail latency.
      hedge_requests_at: 2s
      # -- The maximum number of requests to execute when hedging. Requires hedge_requests_at to be set. Must be greater than 0.
      hedge_requests_up_to: 2

ingester:
  config:
    lifecycler:
      ring:
        # -- Number of copies of snapshots to store in the ingester ring
        replication_factor: 3
        kvstore:
          store: memberlist
      tokens_file_path: /var/deep/tokens.json
    # -- Amount of time a snapshot must be idle before flushing it to the wal.
    snapshot_idle_period: null
    # -- How often to sweep all tenants and move snapshots from live -> wal -> completed blocks.
    flush_check_period: null
    # -- Maximum size of a block before cutting it
    max_block_bytes: null
    # -- Maximum length of time before cutting a block
    max_block_duration: null
    # -- Duration to keep blocks in the ingester after they have been flushed
    complete_block_timeout: null

# To configure a different storage backend instead of local storage:
# storage:
#   tracepoint:
#     backend: s3
#     s3:
#       bucket: <bucketname>
#       access_key: <token>
#       secret_key: <secret>
#       region: eu-west-1
#       endpoint: s3.dualstack.eu-west-1.amazonaws.com
storage:
  tracepoint:
    # -- The supported storage backends are gcs, s3 and azure
    backend: local
    wal:
      path: /var/deep/wal

metricsGenerator:
  enabled: false

memberlist:
  node_name: ""
  randomize_node_name: true
  stream_timeout: "10s"
  retransmit_factor: 2
  pull_push_interval: "30s"
  gossip_interval: "1s"
  gossip_nodes: 2
  gossip_to_dead_nodes_time: "30s"
  min_join_backoff: "1s"
  max_join_backoff: "1m"
  max_join_retries: 10
  abort_if_cluster_join_fails: false
  rejoin_interval: "0s"
  left_ingesters_timeout: "5m"
  leave_timeout: "5s"
  bind_addr: [ ]
  bind_port: 7946
  packet_dial_timeout: "5s"
  packet_write_timeout: "5s"

# Global overrides
global_overrides:
  per_tenant_override_config: /runtime-config/overrides.yaml
  metrics_generator_processors: []

# Per tenants overrides
overrides: |
  overrides: {}

# Set Deep server configuration
server:
  # --  HTTP server listen host
  httpListenPort: 3100
  # -- Log level. Can be set to trace, debug, info (default), warn, error, fatal, panic
  logLevel: info
  # -- Log format. Can be set to logfmt (default) or json.
  logFormat: logfmt
  # -- Max gRPC message size that can be received
  grpc_server_max_recv_msg_size: 4194304
  # -- Max gRPC message size that can be sent
  grpc_server_max_send_msg_size: 4194304
  # -- Read timeout for HTTP server
  http_server_read_timeout: 30s
  # -- Write timeout for HTTP server
  http_server_write_timeout: 30s

# define the config for deep
config:
  # -- Configuration is loaded from the secret called 'externalConfigSecretName'.
  # If 'useExternalConfig' is true, then the configuration is not generated, just
  # consumed.  Top level keys for `tempo.yaml` and `overrides.yaml` are to be
  # provided by the user.
  useExternalConfig: false

  # -- Defines what kind of object stores the configuration, a ConfigMap or a Secret.
  # In order to move sensitive information (such as credentials) from the ConfigMap/Secret to a more secure location (e.g. vault), it is possible to use [environment variables in the configuration](https://grafana.com/docs/mimir/latest/operators-guide/configuring/reference-configuration-parameters/#use-environment-variables-in-the-configuration).
  # Such environment variables can be then stored in a separate Secret and injected via the global.extraEnvFrom value. For details about environment injection from a Secret please see [Secrets](https://kubernetes.io/docs/concepts/configuration/secret/#use-case-as-container-environment-variables).
  storageType: ConfigMap

  # -- Name of the Secret or ConfigMap that contains the configuration (used for naming even if config is internal).
  externalSecretName: '{{ include "deep.resourceName" (dict "ctx" . "component" "config") }}'

  multitenancyEnabled: true

  # -- Structured deep configuration
  structuredConfig: { }
  deep2: |
    multitenancy_enabled: {{ .Values.config.multitenancyEnabled }}

    compactor:
      {{- if .Values.compactor.config }}
        {{- toYaml .Values.compactor.config | nindent 2 }}
      {{- end }}
      ring:
        kvstore:
          store: memberlist

  deep: |
    multitenancy_enabled: {{ .Values.config.multitenancyEnabled }}

    compactor:
      {{- if .Values.compactor.config }}
        {{- toYaml .Values.compactor.config | nindent 2 }}
      {{- end }}
      ring:
        kvstore:
          store: memberlist
    
    distributor:
      ring:
        kvstore:
          store: memberlist
      {{- if .Values.distributor.config }}
        {{- toYaml .Values.distributor.config | nindent 2 }}
      {{- end }}
    
    querier:
      {{- if .Values.querier.config }}
        {{- toYaml .Values.querier.config | nindent 2 }}
      {{- end }}
    
    query_frontend:
      {{- if .Values.queryFrontend.config }}
        {{- toYaml .Values.queryFrontend.config | nindent 2 }}
      {{- end }}

    ingester:
      {{- if .Values.ingester.config }}
        {{- toYaml .Values.ingester.config | nindent 2 }}
      {{- end }}
    
    memberlist:
      {{- with .Values.memberlist }}
        {{- toYaml . | nindent 2 }}
      {{- end }}
      join_members:
        - dns+{{ include "deep.fullname" . }}-gossip-ring:{{ .Values.memberlist.bind_port }}
    
    overrides:
      {{- toYaml .Values.global_overrides | nindent 2 }}
      {{- if .Values.metricsGenerator.enabled }}
      metrics_generator_processors:
      {{- range .Values.global_overrides.metrics_generator_processors }}
      - {{ . }}
      {{- end }}
      {{- end }}
    
    server:
      http_listen_port: {{ .Values.server.httpListenPort }}
      log_level: {{ .Values.server.logLevel }}
      log_format: {{ .Values.server.logFormat }}
      grpc_server_max_recv_msg_size: {{ .Values.server.grpc_server_max_recv_msg_size }}
      grpc_server_max_send_msg_size: {{ .Values.server.grpc_server_max_send_msg_size }}
      http_server_read_timeout: {{ .Values.server.http_server_read_timeout }}
      http_server_write_timeout: {{ .Values.server.http_server_write_timeout }}
    
    storage:
      {{- if .Values.storage }}
        {{- toYaml .Values.storage | nindent 2 }}
      {{- end }}

serviceAccount:
  # -- Specifies whether a ServiceAccount should be created
  create: true
  # -- The name of the ServiceAccount to use.
  # If not set and create is true, a name is generated using the fullname template
  name: null
  # -- Image pull secrets for the service account
  imagePullSecrets: []
  # -- Annotations for the service account
  annotations: {}

metaMonitoring:
  # ServiceMonitor configuration
  serviceMonitor:
    # -- If enabled, ServiceMonitor resources for Prometheus Operator are created
    enabled: true
    # -- Alternative namespace for ServiceMonitor resources
    namespace: null
    # -- Namespace selector for ServiceMonitor resources
    namespaceSelector: {}
    # -- ServiceMonitor annotations
    annotations: {}
    # -- Additional ServiceMonitor labels
    labels: {}
    # -- ServiceMonitor scrape interval
    interval: null
    # -- ServiceMonitor scrape timeout in Go duration format (e.g. 15s)
    scrapeTimeout: null
    # -- ServiceMonitor relabel configs to apply to samples before scraping
    # https://github.com/prometheus-operator/prometheus-operator/blob/master/Documentation/api.md#relabelconfig
    relabelings: []
    # -- ServiceMonitor metric relabel configs to apply to samples before ingestion
    # https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#endpoint
    metricRelabelings: []
    # -- ServiceMonitor will use http by default, but you can pick https as well
    scheme: http
    # -- ServiceMonitor will use these tlsConfig settings to make the health check requests
    tlsConfig: null